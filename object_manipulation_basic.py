"""
FetchReach-v2 + Diffusion Policy (Continuous Action)
WSLì—ì„œë„ ëŒì•„ê°ˆ ìˆ˜ ìˆê²Œ ìµœëŒ€í•œ ê¹”ë”í•˜ê²Œ ì‘ì„±í•œ ì˜ˆì œ.

í•„ìš” íŒ¨í‚¤ì§€ (ì˜ˆì‹œ):
    pip install gymnasium gymnasium-robotics mujoco stable-baselines3 torch numpy tqdm

ì£¼ì˜:
- FetchReach-v2 í™˜ê²½ì´ ì•ˆ ëœ¨ë©´, ì„¤ì¹˜í•œ gymnasium-robotics ë²„ì „ì— ë§ëŠ” env IDë¥¼ í™•ì¸í•´ì•¼ í•¨.
- Mujoco ë¼ì´ì„¼ìŠ¤/ì„¤ì¹˜ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ.
"""

import os
import numpy as np
from dataclasses import dataclass

import gymnasium as gym
from gymnasium.wrappers import FlattenObservation
import gymnasium_robotics

from stable_baselines3 import PPO

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import imageio

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ===========================
# 0. ì„¤ì •
# ===========================
ENV_ID = "FetchPickAndPlace-v4"  # ì„¤ì¹˜ ë²„ì „ì— ë”°ë¼ v1/v3ì¼ ìˆ˜ë„ ìˆìŒ
EXPERT_MODEL_PATH = "ppo_fetchreach_expert"
DATASET_PATH = "fetchreach_expert_dataset.npz"
DIFFUSION_MODEL_PATH = "diffusion_fetchreach_policy.pt"

# ì–´ë–¤ ë‹¨ê³„ê¹Œì§€ ì‹¤í–‰í• ì§€ í”Œë˜ê·¸
TRAIN_EXPERT = True
COLLECT_DATA = True
TRAIN_DIFFUSION = True
EVAL_DIFFUSION = True

print([id for id in gym.envs.registry.keys() if "Fetch" in id])
# In MuJoCo ì„¸ê³„: ë¡œë´‡íŒ”ì„ MJCF íŒŒì¼(XML)ë¡œ â€œì„¤ê³„í•˜ê³ â€ ê·¸ê±¸ ì—”ì§„ì´ ì½ì–´ì„œ ê°€ìƒì˜ ë¡œë´‡íŒ”ì„ ë§Œë“¤ì–´ì¤Œ

def make_env(render_mode): # ë¡œë´‡ íŒ”ì„ ì±…ìƒ ìœ„ì— ê°€ì ¸ë‹¤ ë†“ê³  ì „ì›ì„ ë„£ëŠ” ê²ƒ
    """
    FetchReach-v2 í™˜ê²½ì„ ë§Œë“¤ê³ , observation dictë¥¼ 1D ë²¡í„°ë¡œ flatten.
    """
    env = gym.make(ENV_ID,reward_type="dense", render_mode=render_mode) # ë¡œë´‡ ì‹œë®¬ë ˆì´í„°, MJCF íŒŒì¼ (MuJoCoì˜ ë¡œë´‡/í™˜ê²½ ì •ì˜ íŒŒì¼) ë¶ˆëŸ¬ì˜¤ê³ , ë¬¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™”í•˜ê³ , reward function ì„¸íŒ…í•˜ê³ , action space / observation space ì„¸íŒ…í•˜ê³ 
    
    # MJCF includes ë¡œë´‡ base ìœ„ì¹˜, ê´€ì ˆ(joint) ì¢…ë¥˜: revolute, prismatic ë“±, ê° ë§í¬(link) í¬ê¸°/ì§ˆëŸ‰/ê´€ì„±, ì†(gripper)ì˜ finger geometry, collision shape, actuator (torque/force control) ì„¤ì •, ì¹´ë©”ë¼ ìœ„ì¹˜, ëª©í‘œ(goal) object ìœ„ì¹˜
    
    # FetchReach-v2ì˜ ê¸°ë³¸ observation dict: {
    # "observation": [robot_state vector...],      # ì—°ì† ë²¡í„° (ex. length 10): joint, velocity, gripper, end-effector ìœ„ì¹˜ ë“±
    # "desired_goal": [goal_x, goal_y, goal_z],     # ëª©í‘œ ì¢Œí‘œ ë²¡í„° (ex. length 3)
    # "achieved_goal": [current_x, current_y, current_z]  # í˜„ì¬ end-effector ì¢Œí‘œ (ex. length 3)
    # } => FlattenObservation => obs_flat = [...16 numbers]
    env = FlattenObservation(env)  # dict(obs, desired_goal, achieved_goal) -> flat vector

    # viewer ìƒì„± (ì²« renderì—ì„œ ë§Œë“¤ì–´ì§)
    if render_mode in ("rgb_array", "human"):

        env.reset()
        env.render()  # viewer ì´ˆê¸°í™”

        viewer = env.unwrapped.mujoco_renderer.viewer

        # ===== ì—¬ê¸°ì„œ ì¹´ë©”ë¼ ì„¸íŒ… =====
        viewer.cam.lookat[:] = [2.1, -1.7, 0.1]
        viewer.cam.distance  = 4.3
        viewer.cam.elevation = -30
        viewer.cam.azimuth   = 220
        # =============================

    return env


# ===========================
# 1. Expert (PPO) í•™ìŠµ + ë°ì´í„° ìˆ˜ì§‘
# ===========================
def train_expert(total_timesteps: int = 2_000_000, tuning: bool = True):
    env = make_env(render_mode=None)

    policy_kwargs = dict(
        net_arch=[256, 256],   # ğŸ”§ policy/value ë‘˜ ë‹¤ 256-256
    )

    if tuning and os.path.exists(EXPERT_MODEL_PATH + ".zip"):
        print(f"ğŸ”„ ê¸°ì¡´ Expert ë¡œë“œí•´ì„œ ì´ì–´ì„œ í•™ìŠµ: {EXPERT_MODEL_PATH}.zip")
        model = PPO.load(EXPERT_MODEL_PATH, env=env)
        model.set_env(env)
        model.learn(
            total_timesteps=total_timesteps,
            reset_num_timesteps=False,   # ì´ì–´ì„œ
        )
    else:
        print("ğŸ†• ìƒˆ Expert ëª¨ë¸ ìƒì„±í•´ì„œ ì²˜ìŒë¶€í„° í•™ìŠµ")
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=256,
            gamma=0.98,          # FetchReachì—ì„œëŠ” 0.98 ì •ë„ë„ ì˜ ì‘ë™í•¨
            gae_lambda=0.95,
            ent_coef=0.0,
            vf_coef=0.5,
            max_grad_norm=0.5,
            policy_kwargs=policy_kwargs,
            verbose=1,
        )
        model.learn(total_timesteps=total_timesteps)

    model.save(EXPERT_MODEL_PATH)
    print(f"âœ… Expert model saved to: {EXPERT_MODEL_PATH}.zip")


def collect_expert_data(
    n_episodes: int = 200,
    max_steps_per_episode: int = 50,
):
    """
    í•™ìŠµëœ expert ëª¨ë¸ë¡œ FetchReachë¥¼ ì—¬ëŸ¬ episode í”Œë ˆì´í•˜ë©´ì„œ
    (obs, action) ìŒë“¤ì„ ëª¨ì•„ .npzë¡œ ì €ì¥.

    ë‚˜ì¤‘ì— Behavior Cloning, Diffusion Policy í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ì…‹.
    Both are Imitation Learning, but BC is simpler.
    IL = Strong Supervision
    RL = Weak Supervision (ë˜ëŠ” Sparse Supervision)
    """
    assert os.path.exists(EXPERT_MODEL_PATH + ".zip"), \
        "Expert model not found. ë¨¼ì € train_expert()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."

    env = make_env(render_mode=None)
    model = PPO.load(EXPERT_MODEL_PATH, env=env) # Example

    observations = []
    actions = []

    for ep in range(n_episodes):
        obs, info = env.reset()
        for t in range(max_steps_per_episode):
            # expert í–‰ë™ ì„ íƒ
            action, _ = model.predict(obs, deterministic=True)

            observations.append(obs)
            actions.append(action)

            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            if done:
                break
        # print(f"[Expert Rollout] Episode {ep+1}/{n_episodes} finished at step {t+1}")

    observations = np.array(observations, dtype=np.float32)
    actions = np.array(actions, dtype=np.float32)

    np.savez(DATASET_PATH, observations=observations, actions=actions)
    print(f"âœ… Dataset saved to: {DATASET_PATH}")
    print(f"   observations: {observations.shape}, actions: {actions.shape}")


# ===========================
# 2. Diffusion Policyìš© Dataset
# ===========================
class FetchReachExpertDataset(Dataset):
    """
    (obs, action) supervised í•™ìŠµìš© ë°ì´í„°ì…‹.
    ì—¬ê¸°ì„œëŠ” single-step action diffusion (H=1)ìœ¼ë¡œ ë‘ê³ ,
    ë‚˜ì¤‘ì— H>1 trajectoryë¡œ í™•ì¥ ê°€ëŠ¥.
    """
    def __init__(self, npz_path: str):
        data = np.load(npz_path)
        self.obs = data["observations"]   # shape: (N, obs_dim)
        self.actions = data["actions"]    # shape: (N, act_dim)
        assert self.obs.shape[0] == self.actions.shape[0]

    def __len__(self):
        return self.obs.shape[0]

    def __getitem__(self, idx):
        return self.obs[idx], self.actions[idx]


# ===========================
# 3. Simple DDPM-style Diffusion Model (Action Only, Cond on Obs)
# ===========================
@dataclass
class DiffusionConfig:
    action_dim: int # ë””í“¨ì „ ëª¨ë¸ì´ ì¶œë ¥í•´ì•¼ í•˜ëŠ” continuous vector í¬ê¸°
    obs_dim: int # diffusion modelì´ conditioningí•  ì…ë ¥ ë²¡í„° í¬ê¸°
    timesteps: int = 100       # diffusion steps T
    beta_start: float = 1e-4 # ë² íƒ€(beta) ë¼ëŠ” ë…¸ì´ì¦ˆ í¬ê¸°ë¥¼ stepë§ˆë‹¤ ì¡°ê¸ˆì”© ì¦ê°€ì‹œí‚¤ë©° ë°ì´í„°ë¥¼ ë…¸ì´ì¦ˆí™”
    beta_end: float = 0.02
    hidden_dim: int = 256 # MLP hidden layer size


class TimeEmbedding(nn.Module): # í˜„ì¬ timestep t ë¥¼ ì•Œë ¤ì¤˜ì•¼ í•œë‹¤. t'th denoising step. të¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜, like positional embedding.
    # unique representation for each t is possible.
    """
    ê°„ë‹¨í•œ sinusoidal time embedding + MLP.
    """
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
        self.lin = nn.Sequential(
            nn.Linear(dim, dim),
            nn.ReLU(),
            nn.Linear(dim, dim),
        )

    def forward(self, t: torch.Tensor): # sinusoidal positional encoding (famous)
        """
        t: (B,) integer timesteps
        """
        half_dim = self.dim // 2
        # sinusoidal embedding
        freq = torch.exp(
            torch.arange(half_dim, device=t.device) *
            -(np.log(10000.0) / (half_dim - 1))
        )
        args = t.float().unsqueeze(1) * freq.unsqueeze(0)
        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)
        return self.lin(emb)


class DiffusionPolicy(nn.Module):
    """
    epsilon_theta(x_t, t, obs)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë„¤íŠ¸ì›Œí¬.
    ì…ë ¥: noisy action (x_t), time embedding, obs
    ì¶œë ¥: noise epsilon_hat (action_dim)
    """
    def __init__(self, cfg: DiffusionConfig):
        super().__init__()
        self.cfg = cfg
        self.time_mlp = TimeEmbedding(cfg.hidden_dim)

        in_dim = cfg.action_dim + cfg.obs_dim + cfg.hidden_dim
        h = cfg.hidden_dim

        self.net = nn.Sequential(
            nn.Linear(in_dim, h),
            nn.ReLU(),
            nn.Linear(h, h),
            nn.ReLU(),
            nn.Linear(h, cfg.action_dim),
        )

    def forward(self, x_t, t, obs):
        """
        x_t: (B, action_dim)
        t:   (B,) int64 timesteps
        obs: (B, obs_dim)
        """
        t_emb = self.time_mlp(t)  # (B, hidden_dim)
        x = torch.cat([x_t, obs, t_emb], dim=-1)
        eps_hat = self.net(x)
        return eps_hat # ì›ë˜ actionì— ë”í•´ì§„ noise (eps_hat) ë¥¼ ì˜ˆì¸¡


class ActionDiffusion:
    """
    DDPM-style forward/inverse processë¥¼ ê´€ë¦¬í•˜ëŠ” helper í´ë˜ìŠ¤.
    """
    def __init__(self, cfg: DiffusionConfig):
        self.cfg = cfg

        # beta schedule (linear)
        betas = torch.linspace(cfg.beta_start, cfg.beta_end, cfg.timesteps) # noise
        alphas = 1.0 - betas
        alphas_cumprod = torch.cumprod(alphas, dim=0) # t stepê¹Œì§€ ëˆ„ì í•´ì„œ signal (original data) ì´ ì–¼ë§ˆë‚˜ ë‚¨ì•˜ëŠ”ì§€

        self.register_buffers(betas, alphas, alphas_cumprod)

    def register_buffers(self, betas, alphas, alphas_cumprod):
        self.betas = betas.to(device)
        self.alphas = alphas.to(device)
        self.alphas_cumprod = alphas_cumprod.to(device)
        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)

    def q_sample(self, x0, t, noise=None):
        """
        q(x_t | x_0) = sqrt(alpha_bar_t) x_0 + sqrt(1 - alpha_bar_t) noise
        """
        if noise is None:
            noise = torch.randn_like(x0)

        sqrt_alpha_bar_t = self.sqrt_alphas_cumprod[t].unsqueeze(-1)
        sqrt_one_minus_alpha_bar_t = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(-1)

        return sqrt_alpha_bar_t * x0 + sqrt_one_minus_alpha_bar_t * noise

    def p_sample(self, model: DiffusionPolicy, x_t, t, obs, add_noise: bool = True):
        """
        í•˜ë‚˜ì˜ reverse step: p_theta(x_{t-1} | x_t)
        """
        beta_t = self.betas[t].unsqueeze(-1)          # (B, 1)
        sqrt_one_minus_alpha_bar_t = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(-1)
        sqrt_recip_alpha_t = (1.0 / torch.sqrt(self.alphas[t])).unsqueeze(-1)

        # model predicts epsilon
        eps_theta = model(x_t, t, obs)

        # DDPM ê³µì‹: x_{t-1} = 1/sqrt(alpha_t) * (x_t - (1-alpha_t)/sqrt(1 - alpha_bar_t) * eps_theta) + sigma_t z
        coef = beta_t / sqrt_one_minus_alpha_bar_t
        mean = sqrt_recip_alpha_t * (x_t - coef * eps_theta)
        if add_noise and (t[0] > 0):  # t>0ì¼ ë•Œë§Œ noise (í˜¹ì€ ì•„ì˜ˆ ì œê±°)
            noise = torch.randn_like(x_t)
            sigma_t = torch.sqrt(beta_t)
            x_prev = mean + sigma_t * noise
        else:
            x_prev = mean

        return x_prev

    def p_sample_loop(self, model: DiffusionPolicy, obs, n_samples=1):
        """
        ê´€ì¸¡ obs ì¡°ê±´ì—ì„œ action ìƒ˜í”Œë§.
        single-step action diffusion (H=1).
        """
        model.eval()
        with torch.no_grad():
            # obs: (obs_dim,) -> (1, obs_dim)
            if obs.ndim == 1:
                obs_batch = obs[None, :]
            else:
                obs_batch = obs
            obs_batch = torch.tensor(obs_batch, dtype=torch.float32, device=device)

            B = obs_batch.size(0)
            x_t = torch.randn(B, self.cfg.action_dim, device=device)

            for t_inv in range(self.cfg.timesteps - 1, -1, -1):
                t = torch.full((B,), t_inv, device=device, dtype=torch.long)
                x_t = self.p_sample(model, x_t, t, obs_batch, add_noise=False)

            return x_t.cpu().numpy()  # (B, action_dim)


# ===========================
# 4. Diffusion Policy í•™ìŠµ ë£¨í”„
# ===========================
def train_diffusion_policy(
    batch_size: int = 256,
    epochs: int = 20,
    lr: float = 1e-4,
    resume: bool = True,   # â† ì¶”ê°€
):
    dataset = FetchReachExpertDataset(DATASET_PATH)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)

    # ---- ëª¨ë¸ / cfg ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„± ----
    if resume and os.path.exists(DIFFUSION_MODEL_PATH):
        print(f"ğŸ”„ ê¸°ì¡´ diffusion policy ë¡œë“œí•´ì„œ ì´ì–´ì„œ í•™ìŠµ: {DIFFUSION_MODEL_PATH}")
        checkpoint = torch.load(DIFFUSION_MODEL_PATH, map_location=device)
        cfg_dict = checkpoint["cfg"]
        cfg = DiffusionConfig(**cfg_dict)
        model = DiffusionPolicy(cfg).to(device)
        model.load_state_dict(checkpoint["model_state_dict"])
        diffusion = ActionDiffusion(cfg)
        start_epoch = checkpoint.get("epoch", 0) + 1
    else:
        print("ğŸ†• ìƒˆ diffusion policyë¥¼ ì²˜ìŒë¶€í„° í•™ìŠµ")
        obs_dim = dataset.obs.shape[1]
        act_dim = dataset.actions.shape[1]

        cfg = DiffusionConfig(
            action_dim=act_dim,
            obs_dim=obs_dim,
            timesteps=100,
            beta_start=1e-4,
            beta_end=0.02,
            hidden_dim=256,
        )
        diffusion = ActionDiffusion(cfg)
        model = DiffusionPolicy(cfg).to(device)
        start_epoch = 1

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    mse = nn.MSELoss()

    model.train()
    for epoch in range(start_epoch, start_epoch + epochs):
        total_loss = 0.0
        total_samples = 0

        pbar = tqdm(dataloader, desc=f"[Diffusion Train] Epoch {epoch}")
        for obs_batch, act_batch in pbar:
            obs_batch = obs_batch.to(device)
            act_batch = act_batch.to(device)

            B = obs_batch.size(0)
            x0 = act_batch

            t = torch.randint(0, cfg.timesteps, (B,), device=device).long()
            noise = torch.randn_like(x0)

            x_t = diffusion.q_sample(x0, t, noise=noise)
            eps_hat = model(x_t, t, obs_batch)
            loss = mse(eps_hat, noise)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item() * B
            total_samples += B
            pbar.set_postfix(loss=loss.item())

        avg_loss = total_loss / total_samples
        print(f"[Epoch {epoch}] avg loss: {avg_loss:.6f}")

        # ë§¤ epochë§ˆë‹¤ ë®ì–´ì“°ê¸° ì €ì¥ (epoch ì •ë³´ ê°™ì´ ì €ì¥)
        torch.save(
            {
                "model_state_dict": model.state_dict(),
                "cfg": cfg.__dict__,
                "epoch": epoch,
            },
            DIFFUSION_MODEL_PATH,
        )

    print(f"âœ… Diffusion policy saved to: {DIFFUSION_MODEL_PATH}")
    return model, diffusion, cfg


# ===========================
# 5. Diffusion Policy í‰ê°€
# ===========================
def load_diffusion_policy():
    """
    ì €ì¥ëœ diffusion policy ëª¨ë¸ ë¡œë“œ.
    """
    checkpoint = torch.load(DIFFUSION_MODEL_PATH, map_location=device)
    cfg_dict = checkpoint["cfg"]
    cfg = DiffusionConfig(**cfg_dict)
    model = DiffusionPolicy(cfg).to(device)
    model.load_state_dict(checkpoint["model_state_dict"])
    diffusion = ActionDiffusion(cfg)
    return model, diffusion, cfg

def eval_success(model, n_episodes=50):
    env = make_env(render_mode=None)
    success = 0
    for _ in range(n_episodes):
        obs, info = env.reset()
        done = False
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
        # FetchReachëŠ” info["is_success"]ê°€ ìˆìŒ
        success += info.get("is_success", 0.0)
    env.close()
    print("success_rate:", success / n_episodes)

def evaluate_expert():
    print("ğŸ” Evaluating PPO Expert policy...")
    assert os.path.exists(EXPERT_MODEL_PATH + ".zip"), "Expert not trained yet!"
    env = make_env(render_mode=None)
    model = PPO.load(EXPERT_MODEL_PATH, env=env)
    eval_success(model, n_episodes=50)
    env.close()

def evaluate_diffusion_policy(
    model: DiffusionPolicy,
    diffusion: ActionDiffusion,
    n_episodes: int = 10,
    render: bool = False,
    save_video: bool = True,
    video_dir: str = "videos",
):
    env = make_env(render_mode="rgb_array")
    if render:
        # gymnasium robotics renderëŠ” backendì— ë”°ë¼ ë‹¤ë¦„. ì¼ë‹¨ text infoë§Œ.
        print("âš ï¸ RenderëŠ” mujoco ì„¤ì •ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ë™ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    returns = []
    os.makedirs(video_dir, exist_ok=True)
    success = 0
    for ep in range(n_episodes):
        obs, info = env.reset()
        done = False
        ep_return = 0.0
        step = 0
        frames = []

        while not done and step < 250:
            # obsë¥¼ torch tensorë¡œ ë°”ê¾¸ê³  diffusionìœ¼ë¡œ action ìƒ˜í”Œë§
            obs_tensor = torch.tensor(obs, dtype=torch.float32)
            action = diffusion.p_sample_loop(model, obs_tensor, n_samples=1)[0]

            # FetchReachëŠ” action space boundê°€ ì¡´ì¬. clip í•´ì£¼ëŠ” ê²Œ ì•ˆì „í•¨.
            action = np.clip(action, env.action_space.low, env.action_space.high)

            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            ep_return += reward
            step += 1

            frame = env.render()
            frames.append(frame)
        success += info.get("is_success", 0.0)

        returns.append(ep_return)
        print(f"[Diffusion Policy] Episode {ep+1}/{n_episodes} return: {ep_return:.3f}")

        if save_video and len(frames) > 0:
            # GIFë¡œ ì €ì¥ (ê°„ë‹¨)
            gif_path = os.path.join(video_dir, f"episode_{ep+1:03d}.gif")
            imageio.mimsave(gif_path, frames, fps=15)
            print(f"  ğŸ¥ Saved GIF: {gif_path}")

    # print(f"âœ… Avg return over {n_episodes} episodes: {np.mean(returns):.3f}")
    print(f"[Diffusion] success_rate over {n_episodes} episodes: {success / n_episodes:.3f}")
    env.close()

def record_expert_video(
    n_episodes: int = 5,
    max_steps: int = 50,
    video_dir: str = "videos_expert",
):
    """
    PPO Expert ì •ì±…ì´ ìˆ˜í–‰í•˜ëŠ” ê¶¤ì ì„ GTì²˜ëŸ¼ GIFë¡œ ì €ì¥.
    """
    assert os.path.exists(EXPERT_MODEL_PATH + ".zip"), "Expert not trained yet!"

    os.makedirs(video_dir, exist_ok=True)

    # ë Œë” ê°€ëŠ¥í•œ env
    env = make_env(render_mode="rgb_array")
    model = PPO.load(EXPERT_MODEL_PATH, env=env)

    success = 0

    for ep in range(n_episodes):
        obs, info = env.reset()
        done = False
        frames = []
        step = 0
        ep_return = 0.0

        while not done and step < max_steps:
            # Expert action
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            ep_return += reward
            step += 1

            # ë Œë” í”„ë ˆì„ ì €ì¥
            frame = env.render()
            frames.append(frame)

        success += info.get("is_success", 0.0)
        print(f"[Expert Video] Episode {ep+1}/{n_episodes} return: {ep_return:.3f}, "
              f"is_success: {info.get('is_success', 0.0)}")

        # GIF ì €ì¥
        if len(frames) > 0:
            gif_path = os.path.join(video_dir, f"expert_ep_{ep+1:03d}.gif")
            imageio.mimsave(gif_path, frames, fps=15)
            print(f"  ğŸ¥ Saved Expert GIF: {gif_path}")

    print(f"[Expert] success_rate over {n_episodes} episodes: {success / n_episodes:.3f}")
    env.close()

# ===========================
# Main
# ===========================
if __name__ == "__main__":
    if TRAIN_EXPERT:
        train_expert(total_timesteps=2300_000, tuning=True)
        evaluate_expert()
        record_expert_video(n_episodes=5, max_steps=50, video_dir="videos_expert")

    if COLLECT_DATA:
        collect_expert_data(n_episodes=800, max_steps_per_episode=50)

    if TRAIN_DIFFUSION:
        model, diffusion, cfg = train_diffusion_policy(
            batch_size=256,
            epochs=130,          # ì˜ˆ: 5 epochì”© ì¶”ê°€ë¡œ ë” ëŒë¦¬ê¸°
            lr=1e-4,
            resume=True,       # ì €ì¥ëœ ëª¨ë¸ ìˆìœ¼ë©´ ì´ì–´ì„œ
        )
    else:
        model, diffusion, cfg = load_diffusion_policy()

    if EVAL_DIFFUSION:
        evaluate_diffusion_policy(model, diffusion, n_episodes=50, render=False, save_video=True)